base_model: "meta-llama/Llama-2-7b-chat-hf"
bert_sim: bert-base-nli-mean-tokens
hf_token: "../../hf_token.txt"
eval_contract:
  - ../data/evaluation_contract1.txt
  - ../data/evaluation_contract2.txt
eval_GT:
  - ../data/evaluation_GT1.txt
  - ../data/evaluation_GT2.txt
  
dataset:
  train: ... 
  eval: ...

training:
  output_dir: "../log/SYC_log"
  batch_size: 5
  batch_size_eval: 4
  learning_rate: 1e-4
  epochs: 40
  logging_steps: 30
  save_steps: 500
  fp16: true
  save_model_path: "../pretrained/SYC_stopremoved"

lora:
  r: 16
  alpha: 32
  dropout: 0.1

