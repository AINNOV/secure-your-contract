# Our API keys have been expired :( #

import openai
import os
import numpy as np

with open("../openai_api.txt", "r") as file:
    api_key = file.read().strip()
openai.api_key = api_key

## LLM-as-a-judge ##
def evaluate_with_chatgpt(llama_output, pseudo_gt):
    prompt = f"""
    You have to compare outputs of the negative term/phrase detection and analyasis from the same contracts, generated by Gemini and Ground Truth:

    ### Gemini Output:
    {llama_output}
    
    ###Ground Truth:
    {pseudo_gt}

    Please provide a score from 1 to 10 for how well the Gemini output aligns with the ground truth.
    """
    response = openai.ChatCompletion.create(
    model="gpt-4o",
    messages=[{"role": "system", "content": "You are a helpful evaluation assistant."},
    {"role": "user", "content": prompt}])

    return response['choices'][0]['message']['content']


if __name__ == '__main__':
    generated_dir = "./eval_output/gemini"
    gt_dir = "./data/test/GT"

    generated_files = sorted(os.listdir(generated_dir))
    gt_files = sorted(os.listdir(gt_dir))

    assert len(generated_files) == len(gt_files), "Mismatch in number of contract and GT files"

    generated_texts = []
    references = []

    ## load all the generated answer - gt pairs ##
    for generated_file, gt_file in zip(generated_files, gt_files):
        generated_path = os.path.join(generated_dir, generated_file)
        gt_path = os.path.join(gt_dir, gt_file)

        with open(generated_path, "r") as f:
            generated_text = f.read()
        with open(gt_path, "r") as f:
            gt_text = f.read()

        if gt_text.strip() == '': continue


        generated_texts.append(generated_text)
        references.append(gt_text)

    ## mean score of LLM-as-a-judge from all paris ##
    scores = np.mean([evaluate_with_chatgpt(generated, gt) for generated, gt in zip(generated_texts, gt_text)])
    print("ChatGPT-4o Evaluation Score:\n", scores)